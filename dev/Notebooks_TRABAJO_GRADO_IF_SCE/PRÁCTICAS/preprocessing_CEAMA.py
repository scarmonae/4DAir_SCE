# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import netCDF4 as nc
import glob
import shutil
import os
import lidar
import xarray as xr
from datetime import datetime

from kabl import paths
from kabl import core
from kabl import utils
from kabl import graphics

__author__ = "Carmona-Estrada, Sebastian"
__license__ = "GPL"
__version__ = "1.0"
__maintainer__ = "Carmona-Estrada, Sebastian"
__email__ = "scarmonae@eafit.edu.co"
__status__ = "Production"

def load_pblh_data(data_paths):
    
    """
   PBLH's data reader. 
    
    
    Parameters
    ----------   
    data_paths : list of str
        List of data paths (i.e, ['/drives/c/PBLH*_mm_yy.xlsx'])
        
        
    Returns
    -------   
    data : dict {str: pd.Dataframe}
        Dictionary in which the keys are the instrument's names and the values 
        are pd.Dataframe with the file format.
    """
    

    instruments_names = [dat.split('/')[-1].split('_')[0][4:] for dat in data_paths]
    data = [pd.read_excel(dat) for dat in data_paths]
    data = dict((instruments_names[i], data[i]) for i in range(len(data_paths)))
    
    
    return data

def sort_pblh_data(data: dict):
    
    """
    Sort the PBLH's data such that there are one column for the time in 
    seconds since 1970 and one column for each instrument. returns a 
    Pandas.Dataframe().
    
    
    Parameters
    ----------   
    data : dict of Pandas.DataFrames
        dict of Pandas.DataFrames where the keys are the messure's instrument 
        and the values are the data of the PBLH as Pandas.DataFrame.      
        
    Returns
    -------   
    df : Pandas.Dataframe
        Pandas.DataFrame such that its has 1 column 't_values' for the time
        in seconds since 1970 and one column for each instrument provided in
        the 'data' paramater.
    """
    
    lst = []
    
    for instru in list(data.keys()):
        sorted_data = data[instru].drop('Day', axis=1).transpose().melt()
        sorted_data.rename(columns={'variable': 't_values', 'value': instru}, inplace=True)
        
        for i in range(len(data[instru])):
            sorted_data['t_values'][i * 48] = data[instru]['Day'][i] 
            
        sorted_data['t_values'][sorted_data['t_values'].apply(lambda x: isinstance(x, int))] = np.nan
        
        sorted_data["t_values"] = sorted_data["t_values"].fillna(method='ffill')
        
        for i in range(data[instru].shape[0]):
            for j in range(48):
                sorted_data['t_values'][i*48 + j] = sorted_data['t_values'][i*48 + j].timestamp() + 1800 * (j-1)
        
        lst.append(sorted_data)
        
    df = pd.concat([ i.set_index('t_values') for i in lst], axis=1)
    #df.reset_index(inplace=True)
               
    return df

def reduce_dataset_size(  #Aqui mejor promediar
    dataFiles: list,
    destination: str,
    to_include: list = ['altitude', 
                       'corrected_signal_00',
                       'corrected_signal_02', 
                       'time_bnds', 
                       'time', 'total_rcs532_an', 
                       'range', 
                       'range_resol',
                       'time_resol'
                       ]
    ):
    
    """Reduce the dataset size conserving only the variables especified, in
    order to reduce the memory's demand.
    
    
    Parameters
    ----------
    dataFileS : list of str
        Paths to the data input file, as generated by raw2l1
    
    toinclude : list of str
        Nomes of the variables to conserve.
   
    
    
    Returns
    -------
    """
    
    for file in dataFiles:
        
        day = xr.open_dataset(file)
        day_resampled = day.resample(time='30T').mean()
        
        for key in day_resampled.keys():
            if key not in to_include:
                day_resampled = day_resampled.drop(key)
                
        day_resampled['time'] = day_resampled['time']
        day_resampled.to_netcdf(destination + os.path.basename(file).replace('.nc', '_REDUCED.nc'), encoding={'time':{'units':'days since 1970-01-01'}})
        
        
    
def fit_data(
        dataset: list,
        pbl: str,
        n: int,
        to_extract: list = ['corrected_signal_00', 
                            'corrected_signal_02',
                            'total_rcs532_an']
    ):
    
    """Reduce the dataset size conserving only the variables especified, in
    order to reduce the memory's demand.
    
    
    Parameters
    ----------
    dataFileS : list of str
        Paths to the data input file, as generated by raw2l1aux
    
    toinclude : list of str
        Nomes of the variables to conserve.
   
    
    
    Returns
    -------
    """
    
    pblh = pd.read_csv(pbl)[['t_values', 'MWR']]
    t_values = pd.DataFrame()
    t_values = pd.concat([t_values, pblh.set_index('t_values')], axis=1)
    # t_values.dropna(inplace=True) 
    # t_values.reset_index(inplace=True)
    # t_values.t_values = [datetime.fromtimestamp(x) for x in t_values.t_values]
    # t_values.set_index('t_values', inplace=True)      
    # t_values = t_values.resample('{}T'.format(n)).mean()   #Verificar que en n =30 no cambia el shape().
    # t_values.interpolate(inplace=True)
    # t_values.reset_index(inplace=True)
    # t_values.t_values = [datetime.timestamp(x) for x in t_values.t_values]
    # t_values.set_index('t_values', inplace=True)
    # t_values = t_values.loc[~t_values.index.duplicated(keep='first')]
    
    
    
    
    dsl = dict([(key, pd.DataFrame()) for key in to_extract])
    
    
    for file in dataset:
        t, z, data = utils.extract_data(file, to_extract=to_extract)
        
        t = pd.DataFrame(t, columns=['t_values'])
        
        rcs = data.copy()
        
        for channel in list(rcs.keys()):
            
            rcs[channel] = pd.DataFrame(rcs[channel], columns=z)
            rcs[channel] = pd.concat([t,rcs[channel]], axis=1)
            rcs[channel].set_index('t_values', inplace=True)
            aux = pd.concat([t_values, rcs[channel]], axis=1)
            dsl[channel] = pd.concat([dsl[channel], aux], axis=0)
            
    
    
    for channel in list(rcs.keys()):
            
        #dsl[channel][list(dsl[channel].keys())[1:]] =  dsl[channel].drop(['MWR'], axis=1).interpolate(axis=0, limit=1, limit_area='inside')
        
        
        dsl[channel].dropna(inplace=True)
        
    
    return {key : dsl[key] for key in list(dsl.keys())}, np.array(dsl[list(dsl.keys())[0]]['MWR'])    
    
    
    
def prepare_supervised_dataset(
    dataset: dict,
    ref,
    saveInCSV: bool = False,
    outputFile: str = None,
    plot_on: bool = False,
):
    """Create a dataframe with appropriate fields from original data format.
    
    Lidar data is expected to be provided in raw2l1 files and handmade BLH 
    estimation is expected in .csv file with 2 columns: time, BLH values.
    Paths are given in a list in order to easily had multiple days.
    
    
    Parameters
    ----------
    dataFile : list of str
        Paths to the data input file, as generated by raw2l1
    
    refFile : list of str
        Paths to the reference file (handmade BLH estimation) in CSV format
    
    saveInCSV : bool, default=False
        If True, the dataset is saved in a .csv file at the specified location
        
    outputFile : str, default=None
        Path to the file where the dataset is stored, if saveInCSV=True
    
    plot_on : bool, default=False
        If True, display the handmade BLH over the data.
    
    
    Returns
    -------
    df : `pandas.DataFrame`
        Ready-to-use dataframe for ADABL training. Contains 5 columns of input
        data and one column of output binary data
    """

    RCS0 = []
    RCS1 = []
    RCS2 = []
    SEC0 = []
    ALTI = []
    y = []
    
    
    t_values = list(dataset[list(dataset.keys())[0]].index)
    z_values = list(dataset[list(dataset.keys())[0]].columns[1:])
    dat = {i: np.array(dataset[i].drop('MWR', axis=1)) for i in dataset}
    
    #rcs_0 = dat["total_rcs532_an"]
    rcs_1 = dat["corrected_signal_00"]
    rcs_2 = dat["corrected_signal_02"]
    
    
    blh_ref = ref
    
    
    
    
    if plot_on:
        #paths.resultrootdir='ANTES/kabl-master/results/'
        graphics.blhs_over_data(t_values, z_values, rcs_1, blh_ref)

    # Input data
    # ----------
    #sec_intheday = np.mod(t_values, 24 * 3600)
    sec_intheday = t_values.copy()
    Nt, Nz = rcs_1.shape

    # rcs0loc = rcs_0.ravel()
    # rcs0loc[rcs0loc <= 0] = 1e-5
    # RCS0.append(np.log10(rcs0loc))


    rcs1loc = rcs_1.ravel()
    rcs1loc[rcs1loc <= 0] = 1e-5
    RCS1.append(np.log10(rcs1loc))

    rcs2loc = rcs_2.ravel()
    rcs2loc[rcs2loc <= 0] = 2e-5
    RCS2.append(np.log10(rcs2loc))

    SEC0.append(np.repeat(sec_intheday, Nz))
    ALTI.append(np.tile(z_values, Nt))

    # Output data
    # -----------
    
    yday = []
    for t in range(Nt):
        yloc = np.zeros(Nz)
        yloc[z_values < blh_ref[t]] = 1
        yday.append(yloc)

    y.append(np.array(yday, dtype=int).ravel())

    # Create dataframe
    # ------------------
    df = pd.DataFrame(
        {
            "sec0": np.concatenate(SEC0),
            "alti": np.concatenate(ALTI),
            #"rcs0": np.concatenate(RCS0),
            "rcs1": np.concatenate(RCS1), 
            "rcs2": np.concatenate(RCS2),
            "isBL": np.concatenate(y),
        }
    )

    if saveInCSV:
        if outputFile is None:
            outputFile = paths.file_labelleddataset()
        df.to_csv(outputFile, index=False)
        print("Dataset for ADABL is saved in", outputFile)

    return df


def preprocess_rs_from_nas(
        blhs: str,
        instrument: str = None,
        folder: str = None,
        dst: str = None,
        
    ):
    
    """Reduce the dataset size conserving only the variables especified, in
    order to reduce the memory's demand.
    
    
    Parameters
    ----------
    dataFileS : list of str
        Paths to the data input file, as generated by raw2l1
    
    toinclude : list of str
        Nomes of the variables to conserve.
   
    
    
    Returns
    -------
    """
    nas_path = '/media/nas_gfat/datos/'
    ref = pd.read_csv(blhs#, index_col=0)
    )
    ref.drop(list(ref.columns)[1:], axis=1, inplace=True)
    ref['t_values'] = [ datetime.fromtimestamp(t).date()  for t in ref['t_values']]
    ref = ref[~ref.duplicated(keep='first')]
    ref.reset_index(inplace=True)
    ref.drop('index', axis=1, inplace=True)
    
    rs_data_paths = glob.glob(nas_path + instrument + '/' + folder  + '/**/*Prs*.nc', recursive=True)
    
    usefull_paths = list()
    for date in ref.t_values:
        yy = str(date).split('-')[0]
        mm = str(date).split('-')[1]
        dd = str(date).split('-')[2]
        
        
        for daypath in rs_data_paths:
            if daypath.find(yy+mm+dd) != -1:
                usefull_paths.append(daypath)
                             
                lidar.preprocessing(os.path.dirname(daypath) +'/*Prs*.nc', os.path.dirname(daypath) +'/*Pdc*.nc' ).to_netcdf(dst+
                                                                                                                             'DAILY_MPL_0000_'+
                                                                                                                             yy+mm+dd+
                                                                                                                             '.nc')   
    
    return ref, usefull_paths
    
    
    
def lidar_dataset_preprocess(
        scr: str,
        dst: str,
        
    ):
    
    """Reduce the dataset size conserving only the variables especified, in
    order to reduce the memory's demand.
    
    
    Parameters
    ----------
    dataFileS : list of str
        Paths to the data input file, as generated by raw2l1
    
    toinclude : list of str
        Nomes of the variables to conserve.
    Returns
    -------
    """
    dirs = glob.glob(scr + '*')
    print(dirs)
    
    for day in dirs:
            
        processed = lidar.preprocessing(rs_fl=day+'/*Prs*.nc', dc_fl=day+'/*Pdc*.nc')
        processed.to_netcdf(dst+'DAILY_MPL_0000_{}.nc'.format(day.split('/')[-1]))
        
    return dirs


    
def lidar_dataset_reduce(
        scr: str,
        dst: str = None,
        
    ):
    
    """Reduce the dataset size conserving only the variables especified, in
    order to reduce the memory's demand.
    
    
    Parameters
    ----------
    dataFileS : list of str
        Paths to the data input file, as generated by raw2l1
    
    toinclude : list of str
        Nomes of the variables to conserve.
    Returns
    -------
    """
    dirs = glob.glob(scr + '*.nc')
    reduce_dataset_size(dirs, destination=dst)
        



    
    
        
    
       
        
    
    
    
    
    
    

